version: v1
name: "Ingest Transform Refresh Pipeline"
description: "A comprehensive pipeline that ingests data from multiple sources, transforms it through dbt, and refreshes downstream BI tools"
pipeline:
  ingest_stage:
    tasks:
      ingest_api_data:
        integration: PYTHON
        integration_job: PYTHON_EXECUTE_SCRIPT
        parameters:
          command: python ./ingest/fetch_api_data.py
          package_manager: PIP
          python_version: "3.12"
          build_command: "pip install -r requirements.txt"
          environment_variables: |
            {
              "API_URL": "https://api.example.com/data",
              "API_KEY": "${{ secrets.API_KEY }}"
            }
          project_dir: python/data_ingestion
          shallow_clone_dirs: python/data_ingestion
          set_outputs: true
        depends_on: []
        condition: null
        name: Ingest API Data
        tags: ["ingest", "api"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null

      ingest_database_data:
        integration: POSTGRES
        integration_job: POSTGRES_RUN_QUERY
        parameters:
          statement: |
            SELECT 
              id,
              name,
              email,
              created_at,
              updated_at
            FROM source_users
            WHERE updated_at >= CURRENT_DATE - INTERVAL '1 day';
          set_outputs: true
        depends_on: []
        condition: null
        name: Ingest Database Data
        tags: ["ingest", "database"]
        connection: postgres_source_connection
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null

      ingest_file_data:
        integration: AWS_S3
        integration_job: AWS_S3_DOWNLOAD_FILE
        parameters:
          bucket_name: "data-lake-bucket"
          object_key: "raw-data/daily/users_${{ ORCHESTRA.RUN_DATE }}.csv"
          download_path: "/tmp/users_daily.csv"
          set_outputs: true
        depends_on: []
        condition: null
        name: Ingest File Data from S3
        tags: ["ingest", "s3", "files"]
        connection: aws_s3_connection
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null

      fivetran_rapidly_growing:
        integration: FIVETRAN
        integration_job: FIVETRAN_SYNC_ALL
        parameters:
          connector_id: rapidly_growing
        depends_on: []
        condition: null
        name: Ingest Rapidly Growing Data
        tags: ["ingest", "fivetran"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null

      fivetran_ai_workflow:
        integration: FIVETRAN
        integration_job: FIVETRAN_SYNC_ALL
        parameters:
          connector_id: ai_workflow
        depends_on: []
        condition: null
        name: Ingest AI Workflow Data
        tags: ["ingest", "fivetran"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null

      fivetran_data_pipeline:
        integration: FIVETRAN
        integration_job: FIVETRAN_SYNC_ALL
        parameters:
          connector_id: data_pipeline
        depends_on: []
        condition: null
        name: Ingest Data Pipeline Data
        tags: ["ingest", "fivetran"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null

      fivetran_seed_raising:
        integration: FIVETRAN
        integration_job: FIVETRAN_SYNC_ALL
        parameters:
          connector_id: seed_raising
        depends_on: []
        condition: null
        name: Ingest Seed Raising Data
        tags: ["ingest", "fivetran"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null

      fivetran_tech_company:
        integration: FIVETRAN
        integration_job: FIVETRAN_SYNC_ALL
        parameters:
          connector_id: tech_company
        depends_on: []
        condition: null
        name: Ingest Tech Company Data
        tags: ["ingest", "fivetran"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: null
        configuration: null
    depends_on: []
    condition: null
    name: "Data Ingestion Stage"

  transform_stage:
    tasks:
      run_dbt_models:
        integration: DBT_CORE
        integration_job: DBT_CORE_EXECUTE
        parameters:
          commands: |
            dbt deps
            dbt seed
            dbt run --models +staging
            dbt run --models +intermediate  
            dbt run --models +marts
            dbt test
          package_manager: PIP
          python_version: "3.12"
          project_dir: dbt_projects/analytics
          shallow_clone_dirs: dbt_projects/analytics
          warehouse_identifier: warehouse
          environment_variables: |
            {
              "DBT_PROFILES_DIR": "./profiles",
              "API_DATA": "${{ ORCHESTRA.PIPELINE_RUN_TASKS[\"ingest_api_data\"].OUTPUTS[\"data_path\"] }}",
              "DB_DATA_COUNT": "${{ ORCHESTRA.PIPELINE_RUN_TASKS[\"ingest_database_data\"].OUTPUTS[\"row_count\"] }}"
            }
        depends_on: []
        condition: null
        name: Transform Data with dbt
        tags: ["transform", "dbt", "analytics"]
        connection: warehouse_connection
        operation_metadata: null
        treat_failure_as_warning: false
        configuration: null

      data_quality_checks:
        integration: PYTHON
        integration_job: PYTHON_EXECUTE_SCRIPT
        parameters:
          command: python ./quality/run_data_quality_checks.py
          package_manager: PIP
          python_version: "3.12"
          build_command: "pip install -r requirements.txt"
          environment_variables: |
            {
              "WAREHOUSE_CONNECTION": "${{ connections.warehouse_connection }}",
              "SLACK_WEBHOOK": "${{ secrets.SLACK_WEBHOOK }}"
            }
          project_dir: python/data_quality
          shallow_clone_dirs: python/data_quality
          set_outputs: true
        depends_on: []
        condition: null
        name: Run Data Quality Checks
        tags: ["transform", "quality", "validation"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: false
        configuration: null
    depends_on: ["ingest_stage"]
    condition: null
    name: "Data Transformation Stage"

  refresh_stage:
    tasks:
      refresh_tableau_extracts:
        integration: TABLEAU_CLOUD
        integration_job: TABLEAU_REFRESH_EXTRACT
        parameters:
          project_name: "Analytics"
          datasource_name: "Customer Analytics Dataset"
        depends_on: []
        condition: null
        name: Refresh Tableau Extracts
        tags: ["refresh", "tableau", "bi"]
        connection: tableau_connection
        operation_metadata: null
        treat_failure_as_warning: true
        configuration: null

      refresh_powerbi_datasets:
        integration: POWER_BI
        integration_job: POWER_BI_REFRESH_DATASET
        parameters:
          workspace_id: "analytics-workspace-id"
          dataset_id: "customer-analytics-dataset-id"
        depends_on: []
        condition: null
        name: Refresh Power BI Datasets
        tags: ["refresh", "powerbi", "bi"]
        connection: powerbi_connection
        operation_metadata: null
        treat_failure_as_warning: true
        configuration: null

      update_metabase_cache:
        integration: HTTP
        integration_job: HTTP_REQUEST
        parameters:
          url: "https://metabase.company.com/api/cache/invalidate"
          method: "POST"
          headers: |
            {
              "X-Metabase-Session": "${{ secrets.METABASE_SESSION_TOKEN }}",
              "Content-Type": "application/json"
            }
          body: |
            {
              "include": ["database", "table", "field"]
            }
          set_outputs: false
        depends_on: []
        condition: null
        name: Update Metabase Cache
        tags: ["refresh", "metabase", "cache"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: true
        configuration: null

      send_completion_notification:
        integration: PYTHON
        integration_job: PYTHON_EXECUTE_SCRIPT
        parameters:
          command: python ./notifications/send_completion_alert.py
          package_manager: PIP
          python_version: "3.12"
          build_command: "pip install -r requirements.txt"
          environment_variables: |
            {
              "PIPELINE_STATUS": "SUCCESS",
              "RUN_ID": "${{ ORCHESTRA.RUN_ID }}",
              "QUALITY_CHECK_RESULTS": "${{ ORCHESTRA.PIPELINE_RUN_TASKS[\"data_quality_checks\"].OUTPUTS[\"summary\"] }}",
              "SLACK_WEBHOOK": "${{ secrets.SLACK_WEBHOOK }}",
              "EMAIL_RECIPIENTS": "${{ secrets.EMAIL_RECIPIENTS }}"
            }
          project_dir: python/notifications
          shallow_clone_dirs: python/notifications
          set_outputs: false
        depends_on: []
        condition: null
        name: Send Completion Notification
        tags: ["refresh", "notification", "alerts"]
        connection: null
        operation_metadata: null
        treat_failure_as_warning: true
        configuration: null
    depends_on: ["transform_stage"]
    condition: null
    name: "Refresh & Notification Stage"

schedule:
  - cron: "0 6 * * *" # Daily at 6 AM UTC
    timezone: "UTC"
    enabled: true

sensors: {}
trigger_events: []

webhook:
  enabled: true
  operation_metadata: null
  run_inputs: null

configuration:
  max_concurrent_tasks: 10
  retry_policy:
    max_retries: 3
    retry_delay: 300 # 5 minutes

inputs:
  run_date:
    type: string
    default: "${{ ORCHESTRA.RUN_DATE }}"
    optional: true
    description: "Date for which to run the pipeline (YYYY-MM-DD format)"
  skip_quality_checks:
    type: boolean
    default: false
    optional: true
    description: "Skip data quality validation steps"
  notification_channel:
    type: string
    default: "slack"
    optional: true
    description: "Notification channel (slack, email, or both)"
