version: v1
name: '#dbt #aws #tableau #fivetran'
pipeline:
  e2c76632-fad8-429f-9cad-fd4460309461:
    tasks:
      63e9c0c4-dd90-4df0-829b-9de8ace93ee0:
        integration: FIVETRAN
        integration_job: FIVETRAN_SYNC_ALL
        parameters:
          connector_id: manual_carrier
        depends_on: []
        name: CRM Data
    depends_on: []
    name: ''
  5b15c3c6-7e6a-4e20-bab5-ead456630913:
    tasks:
      4e16058d-7baa-4c2c-b0eb-37a4de1bdb4e:
        integration: AWS_GLUE
        integration_job: AWS_GLUE_RUN_JOB
        parameters:
          job_name: Testing
        depends_on: []
        name: Heavy Ingest
    depends_on: []
    name: ''
  7b344075-c133-47f9-bd1a-7e2573276b46:
    tasks:
      edf8f328-72a0-4dcb-9473-00997c657c18:
        integration: PYTHON
        integration_job: PYTHON_EXECUTE_SCRIPT
        parameters:
          package_manager: PIP
          python_version: '3.12'
          build_command: pip install -r requirements.txt
          environment_variables: '{

            "SHEET_NAME": "${{ MATRIX.gsheets[''SHEET_NAME''] }}",


            "TABLE_NAME":"${{ MATRIX.gsheets[''TABLE_NAME''] }}"


            }'
          set_outputs: false
          source: GIT
          command: python -m run_dlt_pipelines
          project_dir: dlt
          shallow_clone_dirs: dlt
        depends_on: []
        name: API Loads
        connection: python__production__blueprints__19239
    depends_on: []
    name: ''
    matrix:
      inputs:
        gsheets:
        - SHEET_NAME: 1H0UnZ1vJ6WSsZgiVkg96zq52p7qaXkhvodlO1Mzoj6s
          TABLE_NAME: dbt_leads
        - SHEET_NAME: 1TMNhQFZbmaBsa1vIehQzh3vdyfc05damwJ_PIKDF14A
          TABLE_NAME: social_leads
        - SHEET_NAME: 1tImqLq_zLNthL7DDUkTUjU0xHoV_PhdM1zpuJ
          TABLE_NAME: unstructured_feedback
  d2ebbbec-2d12-4221-978c-5dd5acd8041d:
    tasks:
      44bfa9bf-5fc2-4f51-b78f-1c9c14a095d7:
        integration: DBT_CORE
        integration_job: DBT_CORE_EXECUTE
        parameters:
          commands: dbt ${{ inputs.dbt_command }};
          package_manager: PIP
          python_version: '3.12'
          warehouse_identifier: JH88529.UK-SOUTH.AZURE
        depends_on: []
        name: Data Transformations
        connection: dbt_snowflake_56978
        operation_metadata:
          14482e86-07e2-4ffb-a811-36dd9f89c524:
            integration: SNOWFLAKE
            connection: snowflake_tables_24182
    depends_on:
    - 5b15c3c6-7e6a-4e20-bab5-ead456630913
    name: ''
  270869a3-5a06-4c31-9157-8c9c67c99f58:
    tasks:
      93b94371-643b-45ec-a372-fef647337fc4:
        integration: SNOWFLAKE
        integration_job: SNOWFLAKE_RUN_QUERY
        parameters:
          statement: execute task kick_off_task;
        depends_on: []
        name: Execute Task
    depends_on:
    - 7b344075-c133-47f9-bd1a-7e2573276b46
    - e2c76632-fad8-429f-9cad-fd4460309461
    name: ''
  3c0f9149-9236-4831-9269-518b2f6627a0:
    tasks:
      c70b2eae-30e8-4c5a-baa6-7be7f921b3fe:
        integration: POWER_BI
        integration_job: POWER_BI_REFRESH_DATASET
        parameters:
          dataset_id: 5e83dd8a-2c30-44ec-8f53-e5e751ae515d
        depends_on: []
        name: Refresh Dataset
    depends_on:
    - d2ebbbec-2d12-4221-978c-5dd5acd8041d
    - 270869a3-5a06-4c31-9157-8c9c67c99f58
    name: ''
schedule:
- name: Every 3 hours
  cron: 0 */3 ? * * *
  timezone: Europe/London
webhook:
  enabled: false
inputs:
  dbt_command:
    type: string
    default: build
  dbt_branch:
    type: string
    default: main
  backfill_input:
    type: string
    optional: true
meta:
  notes: 'A simple Pipeline used to move data to Snowflake and transform it using
    dbt on a daily schedule.

    '
