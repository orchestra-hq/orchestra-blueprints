version: v1
name: '#Iceberg #GCP #bauplan #dbt #metadata'
pipeline:
  140ed7f2-4e5a-4781-a72f-0a3b6531c88b:
    tasks:
      667649d8-b018-4ad9-842e-5454bb23c620:
        integration: DATAPROC
        integration_job: DATAPROC_RUN_JOB
        parameters:
          cluster_name: quick-cluster
          job_args:
          - '10'
          optional_flags:
            jar_file_uris:
            - file:///usr/lib/spark/examples/jars/spark-examples.jar
            invalid_thing: hello
          job_type: spark
          class_or_jar: org.apache.spark.examples.SparkPi
        depends_on: []
        name: Ingest Data using Spark
      301b1a29-fde8-4b51-b70f-07c0a0b06fb4:
        integration: DATAPROC
        integration_job: DATAPROC_RUN_JOB
        parameters:
          cluster_name: quick-cluster
          job_type: pyspark
          code_location: gs://dataproc-staging-us-central1-214968202047-s64cbcac/hello.py
        depends_on: []
        name: Failed Spark Task
        connection: ${{ ENV.GKE }}
        treat_failure_as_warning: true
    depends_on: []
    name: ''
  db5d92c0-9d84-4658-a841-16125ef410fc:
    tasks:
      677cbba7-0fd1-4a29-8427-e72626d87197:
        integration: DBT_CORE
        integration_job: DBT_CORE_EXECUTE
        parameters:
          commands: dbt build --exclude tag:external_metadata
          package_manager: PIP
          python_version: '3.12'
        depends_on: []
        name: dbt BigQuery
        connection: dbt_core_bigquery_prod_38077
        operation_metadata:
          65837cb3-e3ab-4ffe-ad53-2df79e49d919:
            integration: GCP_BIG_QUERY
            connection: bigquery_metadata_79232
        treat_failure_as_warning: true
      7e834a1f-7c9a-4367-8d42-3acd9ccdbe0e:
        integration: DBT_CORE
        integration_job: DBT_CORE_EXECUTE
        parameters:
          commands: dbt build
          package_manager: PIP
          python_version: '3.12'
        depends_on: []
        name: dbt build
        connection: ${{ ENV.DBT_CORE_CONNECTION }}
      510100b2-bb19-4563-8aed-ab4e610dcdf8:
        integration: DBT
        integration_job: DBT_RUNJOB
        parameters:
          job_id: '123456'
        depends_on: []
        name: Dbt Cloud
        operation_metadata:
          7f011966-eb3a-422d-992e-14d1c9b93c5d:
            integration: SNOWFLAKE
            connection: ${{ ENV.SNOWFLAKE_CONNECTION }}
    depends_on:
    - 140ed7f2-4e5a-4781-a72f-0a3b6531c88b
    name: ''
  9f7538e7-9439-4d60-ba01-b9986ae628d4:
    tasks:
      77301c3f-7dac-458e-b3d9-6cb8fee6774f:
        integration: GCP_CLOUD_RUN
        integration_job: GCP_CLOUD_RUN_EXECUTE_JOB
        parameters:
          job_name: run
        depends_on: []
        name: API data
      bffbaaf3-c004-4e44-b90e-2e23802369e8:
        integration: GCP_DATAFLOW
        integration_job: GCP_DATAFLOW_RUN_PIPELINE
        parameters:
          pipeline_name: ingest-api-to-bigquery
        depends_on: []
        name: Ingest SFTP
    depends_on: []
    name: ''
  3bfc92c0-b0d7-459a-998b-9782cc6dfc1b:
    tasks:
      16f6dad2-8a14-49d5-852f-8ee96e7f4553:
        integration: PYTHON
        integration_job: PYTHON_EXECUTE_SCRIPT
        parameters:
          package_manager: PIP
          python_version: '3.12'
          build_command: pip install -r requirements.txt
          environment_variables: '{

            "SHEET_NAME": "${{ MATRIX.gsheets[''SHEET_NAME''] }}",


            "TABLE_NAME":"${{ MATRIX.gsheets[''TABLE_NAME''] }}"


            }'
          set_outputs: false
          source: GIT
          command: python -m run_dlt_pipelines
          project_dir: dlt
          shallow_clone_dirs: dlt
        depends_on: []
        name: SFTP Data
        connection: python__production__blueprints__19239
    depends_on:
    - 5218723c-6b9c-4114-8312-fb61801ce150
    name: ''
    matrix:
      inputs:
        gsheets: ${{ ORCHESTRA.PIPELINE_RUN_TASKS['02ae7a6c-d302-4ae2-b05e-8d50d59e9753'].OUTPUTS['results']
          }}
  42cebbe6-61b1-43f7-9101-1886b4a773f6:
    tasks:
      f34c3045-dea9-4a50-a8b0-ea496db4cd3f:
        integration: TABLEAU_CLOUD
        integration_job: TABLEAU_REFRESH_EXTRACT
        parameters:
          project_name: southwire
          datasource_name: ${{ MATRIX.extracts }}
        depends_on: []
        name: Refresh Extracts
    depends_on:
    - db5d92c0-9d84-4658-a841-16125ef410fc
    - 59b47850-b4e7-4b3f-9550-6bbecb79c4af
    - a64ca2d9-0f87-47a4-933d-72ded475cf6d
    name: ''
    matrix:
      inputs:
        extracts:
        - extract_1
        - extract_2
        - extract_3
        - extract_4
  a64ca2d9-0f87-47a4-933d-72ded475cf6d:
    tasks:
      1215c44a-ec8e-4df1-9072-adf6170912be:
        integration: BAUPLAN
        integration_job: BAUPLAN_EXECUTE_PIPELINE
        parameters:
          project_dir: bauplan/
          bauplan_ref: orchestradevs.oliver_test
          project_branch: test-bauplan
          namespace: bauplan
        depends_on: []
        name: Crunch with Bauplan
    depends_on:
    - 9f7538e7-9439-4d60-ba01-b9986ae628d4
    name: ''
  59b47850-b4e7-4b3f-9550-6bbecb79c4af:
    tasks:
      f9746b15-accb-48bb-bc2b-d0f691596752:
        integration: GCP_BIG_QUERY
        integration_job: GCP_BQ_RUN_SCHEDULED_QUERY_JOB
        parameters:
          resource_name: projects/249301810188/locations/europe-west1/transferConfigs/66e02643-0000-2d9b-b72a-582429cb0ea4
        depends_on: []
        name: Scheduled Query
        connection: bigquery__scheduled_query__prod__64336
    depends_on:
    - 3bfc92c0-b0d7-459a-998b-9782cc6dfc1b
    name: ''
  5218723c-6b9c-4114-8312-fb61801ce150:
    tasks:
      02ae7a6c-d302-4ae2-b05e-8d50d59e9753:
        integration: POSTGRES
        integration_job: POSTGRES_RUN_QUERY
        parameters:
          statement: select * from config_table
          set_outputs: true
        depends_on: []
        name: Get Metadata
    depends_on: []
    name: ''
webhook:
  enabled: false
